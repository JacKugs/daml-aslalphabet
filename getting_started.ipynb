{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-04 15:19:19.520033: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "## packages installed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import seaborn\n",
    "import keras\n",
    "import mediapipe as mp \n",
    "import pathlib\n",
    "import cv2\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Load the ASL Alphabet dataset, using keras.preprocessing.image \n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "# train_data= pathlib.Path(os.path.expanduser(\"~/Desktop/DAML_ASL/archive/asl_alphabet_train/asl_alphabet_train\")).with_suffix('')\n",
    "# train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "#   train_data,\n",
    "#   validation_split=0.2,\n",
    "#   subset=\"training\",\n",
    "#   seed=123)\n",
    "\n",
    "# val_ds = tf.keras.utils.image_dataset_from_directory(train_data,\n",
    "#   validation_split=0.2,\n",
    "#   subset=\"validation\",\n",
    "#   seed=123)\n",
    "data_dir=pathlib.Path(os.path.expanduser(\"~/Desktop/DAML_ASL/archive\")).with_suffix('')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' train_ds.class_names\\nimport matplotlib.pyplot as plt\\nplt.figure(figsize=(10, 10))\\nfor images, labels in train_ds.take(1):\\n  for i in range(9):\\n    ax = plt.subplot(3, 3, i + 1)\\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\\n    plt.title(train_ds.class_names[labels[i]])\\n    plt.axis(\"off\") '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" train_ds.class_names\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(train_ds.class_names[labels[i]])\n",
    "    plt.axis(\"off\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MediaPipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check https://developers.google.com/mediapipe/solutions/vision/hand_landmarker/python for guidance on this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO: Download and Load Model Path\n",
    "\n",
    "model_path = os.path.expanduser('~/Desktop/DAML_ASL/daml-aslalphabet/hand_landmarker.task')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jackkugler/Desktop/DAML_ASL/daml-aslalphabet/hand_landmarker.task\n"
     ]
    }
   ],
   "source": [
    "## TO-DO: prepare image data, create task, and run the task\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "HandLandmarker = mp.tasks.vision.HandLandmarker\n",
    "HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "print(model_path)\n",
    "options = HandLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n",
      "235\n",
      "235\n",
      "188\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_output(landmarks, Y):\n",
    "    removal =[]\n",
    "    Y_new = Y\n",
    "    return_output = []\n",
    "    print(len(landmarks))\n",
    "    count = 0\n",
    "    for i in range(len(landmarks)):\n",
    "        return_data = []\n",
    "        if len(landmarks[i].hand_landmarks) == 0:\n",
    "            removal.append(i)\n",
    "            continue\n",
    "        for NormaLandmark in landmarks[i].hand_landmarks[0]:\n",
    "            return_data.append(NormaLandmark.x)\n",
    "            return_data.append(NormaLandmark.y)\n",
    "        return_output.append(return_data)\n",
    "    for x in reversed(removal):\n",
    "       Y_new.pop(x)\n",
    "    return return_output, Y_new\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    extract and clean landmark coordinate data: including \n",
    "    - Removing depth (or z dimension)\n",
    "    - Centralizing coordinates to center point of hand\n",
    "    - Flattening to 1D array \n",
    "    - Normalization w.r.t. max value \n",
    " \n",
    "    Paramaters\n",
    "    ---------\n",
    "    landmarks: output from Mediapipe model \n",
    "    \"\"\"\n",
    "Y_train = []\n",
    "X_train = []\n",
    "Y_test = []\n",
    "X_test = []\n",
    "alphabet = list(string.ascii_uppercase)\n",
    "with HandLandmarker.create_from_options(options) as landmarker:\n",
    "    for letter in alphabet:\n",
    "        count = 0 \n",
    "        #print(letter)\n",
    "        for images in data_dir.glob(f'*/asl_alphabet_train/{letter}/*.jpg'):\n",
    "            #letter = str(images).split('asl_alphabet_train/asl_alphabet_train/',1)[1][0]\n",
    "            Y_train.append(letter)\n",
    "            mp_image = mp.Image.create_from_file(str(images))\n",
    "            hand_landmarker_result = landmarker.detect(mp_image)\n",
    "            X_train.append(hand_landmarker_result)\n",
    "            count+=1\n",
    "            if count>10:\n",
    "                break\n",
    "# count = 0 \n",
    "# with HandLandmarker.create_from_options(options) as landmarker:\n",
    "#     for images in data_dir.glob('*/asl_alphabet_test/*/*.jpg'):\n",
    "#         letter = str(images).split('asl_alphabet_test/asl_alphabet_test/',1)[1][0]\n",
    "#         Y_test.append(letter)\n",
    "#         #print(\"Here\")\n",
    "#         mp_image = mp.Image.create_from_file(str(images))\n",
    "#         mp_image = mp.Image.create_from_file(str(images))\n",
    "#         hand_landmarker_result = landmarker.detect(mp_image)\n",
    "#         X_test.append(hand_landmarker_result)\n",
    "#         count +=1\n",
    "#         if count > 100:\n",
    "#             break\n",
    "\n",
    "markers, letters = (process_output(X_train,Y_train))\n",
    "print(len(markers))\n",
    "print(len(letters))\n",
    "X_trainMP, X_testMP, Y_trainMP, Y_testMP = train_test_split(markers, letters,test_size=0.2, random_state=123)\n",
    "print(len(X_trainMP))\n",
    "print(len(Y_trainMP))\n",
    "# X_test, Y_test = process_output(X_test, Y_test)\n",
    "#data_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable ellipsis object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m##TO-DO: generate the train and test set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable ellipsis object"
     ]
    }
   ],
   "source": [
    "\n",
    "##TO-DO: generate the train and test set\n",
    "#X_train, y_train, X_test, y_test =  sklearn.model_selection.train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training with ResNet / VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO-DO: train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training with Mediapipe Landmarks (Random Forest, LSTM, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3404255319148936"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TO-DO: train and evaluate model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 500, max_depth = 4, max_features = 3, bootstrap = True, random_state = 18).fit(X_trainMP, Y_trainMP)\n",
    "# Create our predictions\n",
    "prediction = rf.predict(X_testMP)\n",
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "confusion_matrix(Y_testMP, prediction)\n",
    "# Display accuracy score\n",
    "#accuracy_score(Y_testMP, prediction)\n",
    "# Display F1 score\n",
    "#f1_score(Y_testMP,prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAML_ASL",
   "language": "python",
   "name": "daml_asl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dc7617740451ba40477e2608e8f871cd66ec60ad5d12418ee7d364cd87e25c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
